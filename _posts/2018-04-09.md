---
title: Perpetuum Modelle
author: Mike Kroutikov
layout: post
published: false
----

We need perpetual model training!

Here is a picture of a happy Machine Learning Developer. Lets call him Bob. Bob is happy, because
his ML model works well: he found the best hyperparameters, uses right capacity and gets the expected
results.

Now Bob's main task is to maintain the performance of his model. Life changes, and data also changes.
New data may be a bit different than the one model was trained on. But Bob is well aware of this and
he not only regularly monitors the model performance, he is also re-training his model on a regular 
schedule.

Here is how this works:
1. Every week Bob takes a snapshot of the current feedback data and creates a new training dataset.
2. Then Bob starts his training task. Its a long one and takes 2 days.
3. Once model is trained, Bob does evaluation to ensure that it performs at least as good as the one
   currently deployed.
4. Finally, the model is deployed.

But there is something not quite right in this picture. First, every model is re-trained from scratch
and old model is thrown out. Second, deployed model has a significant lag (about a week) comparing
to the data. And third, we can not train model for longer time - that will delay the deployment schedule.

Bob's company is putting in place a new incentive plan. Bob's bonus now directly depends on his model 
performance. This makes Bob wonder: can we do better? Hmm.. Hmm...

[picture of Bob as a Thinker]

Yes, we can (and Bob can too)! We can address both performance concerns by getting rid of the static
training datasets, and start training continuously:

1. we start out training with current pre-trained model
2. we feed data from dynamic repository (e.g. a specific folder). In the training loop the data
   repository is re-evaluated every few minutes to pick up new files (if available)
3. learning rate is no longer on a declining schedule, but is fixed (and can be adjusted at runtime,
   if needed)
4. amount of dropout in the network is also dynamic and can be adjusted at runtime (to combat overfitting)
5. every hour or so, we take a snapshot of the trained network (TF fan will call it "checkpoint"), do
   proper evaluation (automatically).
6. every day we pick the best performing checkpoint from the list of backlog

This modus operandi creates few interesting research items:
* dynamic dataset: at runtime, dataset files can be added and removed - easy
* what is the best value of learning rate - so that new information is mixed in reasonably quickly, but
  without introducing too much minibatch noise in the training
* do we need to vary dropout? As more data training becomes available, we may want to scale dropout down?
* what about vocabularies? New data may bring never seen terms. How can we evolve word (and sub-word) vocabularies
  to allow dynamic change?
* what if new batch of data is "spoiled"? Meaning that is contains some corrupted data? Looks like we need a 
  mechanism to restart continuous training from one of the previous snapshots.
  
Lets hope that Bob will figure all this out and get hit fat bonus!
  
